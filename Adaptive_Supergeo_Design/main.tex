\documentclass[final,3p,fleqn, 10pt]{elsarticle}
\input{preamble}
\begin{document}
\begin{frontmatter}
\title{Adaptive Supergeo Design: A Scalable Framework for Geographic Marketing Experiments}
\author{Charles Shaw. \\This version: \today}
\date{this version: \today}

  \begin{abstract}
Geographic experiments are a gold-standard for measuring incremental return on ad spend (\textit{iROAS}) at scale, yet their design is challenging: the unit count is small, heterogeneity is large, and the optimal Supergeo partitioning problem is NP-hard. We introduce \emph{Adaptive Supergeo Design} (ASD), a two-stage framework that renders Supergeo designs practical for thousands of markets. A bespoke graph-neural network first learns geo-embeddings and proposes a concise candidate set of “supergeos”; a CP-SAT solver then selects a partition that balances both baseline outcomes and pre-treatment covariates believed to modify the treatment effect. We prove that ASD’s objective value is within $(1+\varepsilon)$ of the global optimum under mild community-structure assumptions. In simulations with up to 1\,000 Designated Market Areas ASD completes in under a minute on a single core, retains every media dollar, and cuts iROAS bias substantively relative to existing methods. ASD therefore turns geo-lift testing into a routine, scalable component of media planning while preserving statistical rigour.
\vspace{10pt}

Keywords: Marketing Science, Advertising, Causal Inference, Geographic Experiments, Incremental ROAS, Graph Neural Networks, Combinatorial Optimisation
\end{abstract}



\end{frontmatter}


\section*{Executive Summary for Marketing Leaders.} Adaptive Supergeo Design (ASD) enables marketers to run clean geo\,-lift tests without sacrificing markets or long waits for optimisation. By automatically grouping Designated Market Areas (DMAs) into balanced ``supergeos'', ASD (1) maximises geographic coverage—no blackout markets—so every ad dollar still works, (2) cuts design runtime substantively, enabling same-day go/\,no\,-go decisions, and (3) delivers tighter incremental return on ad spend (iROAS) estimates, reducing bias by approximately 30\,\% relative to current practice. ASD therefore turns geo-experimentation into a routine, scalable component of media planning.



\section{Introduction}
\label{sec:introduction}
Measuring the causal impact of large-scale advertising campaigns is a fundamental challenge for marketers and media scientists. Geographic experiments where the units of randomisation are entire regions such as cities or designated market areas have emerged as a gold standard methodology. Such experiments are often preferred over individual-level randomisation because they are robust to network spillover effects and can be implemented without relying on sensitive user-level data which is increasingly restricted by privacy regulations. Modern applications further demand an understanding that goes beyond simple averages to capture the full distributional impact of a campaign. These designs allow practitioners to measure the aggregate effect of a campaign or policy in a real-world setting providing valuable insights for decision making. Throughout this paper we denote a market's weekly revenue by $R$, media spend by $S$, and define incremental return on ad spend (iROAS) as $\Delta R / \Delta S$, where $\Delta R$ denotes the post\,-period revenue uplift relative to the control group and $\Delta S$ is the corresponding uplift in spend. When we refer to \emph{response} and \emph{spend}, we specifically mean revenue and media cost, respectively.

Despite their conceptual power geographic experiments present formidable statistical and computational hurdles. The number of available geographic units is typically small which limits statistical power and makes achieving balance between treatment and control groups difficult. Furthermore these units often exhibit extreme heterogeneity. A small number of large metropolitan areas may account for a substantial portion of the overall response which leads to heavy-tailed distributions that violate the assumptions of classical statistical models. While modern analysis methods have evolved to estimate complex distributional treatment effects the design methodologies needed to produce data suitable for them have lagged behind. Designing an experiment that is both balanced and powerful in this low-N high-heterogeneity environment is a non-trivial task and the problem of finding an optimal experimental design is often a complex combinatorial challenge that can be computationally intractable.

Significant progress has been made in developing specialised methods to address these issues. The Trimmed Match Design framework introduced a robust approach to analysis by systematically trimming outlier pairs to protect against the influence of extreme heterogeneity. A subsequent innovation the Supergeo Design recognised that trimming could introduce bias when treatment effects are themselves heterogeneous. It proposed a generalised matching framework where multiple geos could be grouped into larger supergeos to find better matches without discarding any units. While this elegant solution provides a path to unbiased estimation of the average treatment effect it does so at the cost of transforming the design problem into an NP-hard optimisation task rendering it computationally infeasible for all but the smallest of applications.

This paper introduces the Adaptive Supergeo Design or ASD a novel framework that retains the statistical benefits of the Supergeo concept while overcoming its computational limitations. Our contribution is twofold. First we propose a scalable two-stage heuristic algorithm that makes the design of large-scale geographic experiments computationally practical. This method uses a bespoke graph neural network to learn geo-embeddings that capture complex relationships which are then used to generate high-quality candidate supergeos. This reduces the intractable partitioning problem to a manageable optimisation task. Second we introduce a heterogeneity-aware objective function for the design process. This function explicitly balances on observable characteristics that are likely to modify the treatment effect leading to more robust and credible causal estimates.

The remainder of this paper is structured to fully detail our proposed methodology. We begin by formalising the problem of experimental design within the potential outcomes framework. We then present the technical details of the Adaptive Supergeo Design methodology including the graph neural network architecture and the heterogeneity-aware optimisation. Subsequently we provide theoretical arguments for the quality of our heuristic approach. The performance of our method is then evaluated through a series of simulation studies that test its scalability and robustness under various conditions.  Finally we conclude with a discussion of the findings and suggest avenues for future research.

\section{Literature Review}
\label{sec:literature}
The design of geographic experiments has evolved to address the statistical challenges posed by small sample sizes and significant heterogeneity. A common approach to improve the precision of experimental results is the matched-pairs design \citep{imbens2015causal}. In this framework units are paired based on similarity across pre-treatment covariates and randomisation is performed within each pair. The intuition is that differencing the outcomes of similar units will cancel out much of the baseline variance making the treatment effect easier to detect. The problem of finding the optimal set of pairs can be formulated as a minimum-weight matching problem which can be solved efficiently in polynomial time \citep{edmonds1965maximum, rosenbaum2020design}.

A practical difficulty arises when some geographic units are outliers and cannot be matched well with any other unit. The Trimmed Match Design framework \citep{chen2021} addresses this issue by proposing to trim or discard these poorly matched pairs from the experiment. This can improve the precision of the resulting treatment effect estimate for the remaining units. A subsequent and crucial insight however is that this trimming process can introduce bias if the treatment effect itself is heterogeneous. If the trimmed units have systematically different treatment effects from the rest of the population then the estimate derived from the trimmed sample will not be representative of the average treatment effect for the entire population.

The Supergeo Design was proposed to combat this issue of poor matches without sacrificing data \citep{chen2023}. It introduces a generalised matching problem where multiple individual geos can be combined to form a composite supergeo. The goal is then to partition the entire set of geos into two balanced groups of supergeos. This allows for better matches to be found for all units including outliers thus avoiding the need for trimming. While this provides a path to an unbiased estimate of the population average treatment effect it comes at a significant computational cost. The Supergeo design problem is NP-hard making it computationally infeasible for all but the smallest applications and motivating the need for a new scalable approach.

Recent work within \emph{marketing science} has revisited geo\,-level experimentation from an industry perspective. \citet{gordon2021advertising} compare matched--pair tests, synthetic controls, and causal forests across hundreds of advertiser\,–market combinations and show that geo experiments dominate in bias--variance trade\,off when the number of markets is moderate. Building on this insight, Facebook's open-source \texttt{GeoLift} tool describes an automated holdout allocation and power analysis solution for advertisers at scale. In the linear television domain, practitioners have deployed a test--control methodology across all U.S. Designated Market Areas while respecting weekly spend constraints.

These production systems rely on either random assignment or heuristic pairing and frequently resort to \emph{blackout} rules that exclude problematic markets. ASD instead optimises a balanced partition while preserving \emph{every} market, delivering greater coverage and statistical precision.

A separate strand of literature infers incremental impact without randomisation via marketing\,–mix models (MMM) or causal forecasters (e.g., practitioner white papers). While MMMs estimate long\,-run elasticities from observational time series, their validity rests on strong identifiability assumptions and careful prior specification. ASD is complementary: it provides high\,-quality experimental data that can feed into MMMs or serve as a ground\,-truth benchmark, thus closing the loop between experimentation and modelling.

This evolution in experimental design runs parallel to an evolution in experimental analysis. The development of powerful techniques such as Double/Debiased Machine Learning \citep{chernozhukov2018debiased} has increased the demand for experimental data that is rich in covariate information. These modern analysis methods move beyond simple baseline comparisons and leverage detailed pre-treatment characteristics to improve the precision and robustness of causal estimates. This creates a clear need for design methodologies that can proactively create balance not just on past outcomes but on the full vector of covariates that will be used in the final analysis.

\section{Background and Problem Formulation}
\label{sec:background}
To formally ground our methodology we adopt the potential outcomes framework commonly used in causal inference. This framework allows us to define causal effects precisely even though they are not directly observable.

    \subsection{The Potential Outcomes Framework for Geographic Experiments}
    Let us consider a set of $N$ geographic units available for an experiment. For each unit $i \in \{1, \dots, N\}$, we define two potential outcomes. Let $Y_i(1)$ be the outcome for unit $i$ if it is assigned to the treatment group and let $Y_i(0)$ be the outcome if it is assigned to the control group. The fundamental problem of causal inference is that for any given unit $i$ we can only ever observe one of these two potential outcomes. The individual causal effect for unit $i$ is defined as $\tau_i = Y_i(1) - Y_i(0)$.

    While the Average Treatment Effect or ATE defined as $\tau_{ATE} = \mathbb{E}[\tau_i]$ is a common summary measure it can mask significant underlying heterogeneity. A more complete picture of a campaign's impact is provided by the Distributional Treatment Effect or DTE. The DTE is the difference between the cumulative distribution functions (CDFs) of the potential outcomes $F_{Y(1)}(y) - F_{Y(0)}(y)$. It describes how the treatment shifts the entire distribution of outcomes.

    The presence of treatment effect heterogeneity can be formalised through the Conditional Average Treatment Effect or CATE defined as $\tau_{CATE}(x) = \mathbb{E}[\tau_i | X_i = x]$ for a vector of pre-treatment covariates $X_i$. A primary goal of modern experimental design is to enable the precise and unbiased estimation of both the ATE and the broader DTE particularly when the CATE function is non-trivial.

    \subsection{Limitations of Standard Designs}
    The simplest experimental design is complete randomisation where each of the $N$ units is assigned to treatment or control with equal probability subject to group size constraints. While this approach is unbiased in expectation any single randomisation can result in substantial covariate imbalance between the treatment and control groups especially when the number of units $N$ is small. Such imbalance inflates the variance of the ATE estimator reducing the statistical power of the experiment.

    To mitigate this issue matched-pair designs are frequently employed. In this approach units are paired based on their similarity across pre-treatment covariates. Randomisation then occurs within each pair. By differencing the outcomes of the two units within a pair much of the variance attributable to the matching covariates is eliminated. This generally leads to a more precise estimate of the ATE. The matched-pair design serves as a foundational concept for more advanced methods that seek to optimise the quality of matches.

    \subsection{The Supergeo Design Problem}
    The Supergeo framework generalises the idea of matched pairs. Instead of a one-to-one matching it seeks to partition the entire set of $N$ geos $\mathcal{G}$ into two groups a treatment group $\mathcal{G}_T$ and a control group $\mathcal{G}_C$ that are as similar as possible. These groups may be composed of complex combinations of the original units. For instance the treatment group could be a collection of disjoint subsets of $\mathcal{G}$ called supergeos $\mathcal{G}_T = \{S_{T1}, S_{T2}, \dots, S_{Tk}\}$ and similarly for the control group.

    The objective is to find a partition that minimises the imbalance across a set of pre-treatment characteristics. For a single baseline characteristic $R$, the objective function from the original Supergeo paper can be represented as minimising the total difference between the two groups: $\min |\sum_{i \in \mathcal{G}_T} R_i - \sum_{j \in \mathcal{G}_C} R_j|$. In its full form, the problem involves balancing a vector of characteristics, which further contributes to its NP-hard nature. This computational barrier makes it impractical for experiments with more than a few dozen units and motivates the development of a scalable and effective heuristic which is the central focus of our work.

    \subsection{State-of-the-Art Estimation of Distributional Effects}
    Recent advances in econometrics provide a powerful method for estimating DTEs at the analysis stage. The approach proposed by Oka et al. (2024) and extended by Byambadalai et al. (2024) uses regression adjustment to improve the precision of DTE estimates. The core idea is to first model the conditional outcome distribution $F_{Y|X}(y|x)$ using flexible machine learning methods and cross-fitting. The final DTE estimate is then constructed based on a Neyman-orthogonal moment condition that is robust to moderate errors in the machine learning model. This technique represents the state-of-the-art for DTE analysis. Given that these powerful regression-adjustment techniques will be used for analysis, the design phase should no longer focus solely on balancing baseline outcomes. Instead, the design itself must proactively create balance on the very covariates ($X$) that the analysis-stage model will use, thereby maximizing the precision of the final DTE estimate. This is a key motivation for our work.

\section{The Adaptive Supergeo Design Methodology}
\label{sec:methodology}
The core of our contribution is the Adaptive Supergeo Design or ASD a methodology that renders the Supergeo design problem computationally tractable and explicitly accounts for treatment effect heterogeneity. We overcome the NP-hard nature of the problem by decomposing it into two sequential stages a candidate generation stage and a final partitioning stage.

    \subsection{A Two-Stage Heuristic for Scalable Design}
    Finding the optimal partition of $N$ geos into two balanced groups is computationally infeasible at scale. Our central insight is to avoid searching the entire space of possible partitions. Instead we first use a machine learning model to learn the underlying structure of the geographic data and identify a much smaller set of high-quality candidate supergeos. In the second stage we solve a now manageable optimisation problem to find the best final partition using only these candidates. This two-stage approach transforms an intractable problem into a practical and scalable workflow.

    \subsection{Stage 1: Candidate Supergeo Generation via Graph Neural Networks}
    The first stage aims to produce a rich set of plausible supergeos by learning the complex relationships between geographic units. We model the set of geos as a graph $\mathcal{G} = (V, E, X)$ where $V$ is the set of $N$ geo nodes, $E$ is a weighted adjacency matrix representing inter-geo relationships (such as proximity or travel time), and $X$ is a matrix of node features. The features for each node $i$, $X_i$, are engineered to include static demographics and a rich set of time series characteristics. For example, using libraries such as `tsfresh`, we extract features like the mean, variance, and Fourier coefficients of the weekly response time series to capture its level, volatility, and seasonality.

    We then employ a hybrid Graph Attention (GAT) and GraphSAGE model to learn an embedding $h_i \in \mathbb{R}^d$ for each geo $i$. The embedding for a node at layer $(l+1)$ is generated by
    $$ h_i^{(l+1)} = \sigma\left(W \cdot \text{CONCAT}\left(h_i^{(l)}, \sum_{j \in \mathcal{N}(i)} \alpha_{ij} h_j^{(l)}\right)\right) $$
    where $\mathcal{N}(i)$ is the neighbourhood of node $i$, $\alpha_{ij}$ are learned attention weights from the GAT mechanism, $W$ is a trainable weight matrix, and $\sigma$ is a non-linear activation function. The GNN is trained using a multi-task self-supervised objective function
    $$ \mathcal{L}_{GNN} = \mathcal{L}_{\text{contrastive}} + \gamma \mathcal{L}_{\text{regression}} $$
    where $\mathcal{L}_{\text{contrastive}}$ is a contrastive loss that pulls similar geos closer in the embedding space and $\mathcal{L}_{\text{regression}}$ is a secondary loss that encourages the embeddings to be predictive of future baseline outcomes.

    \paragraph{Implementation Details.} Our final architecture comprises two GAT layers (8 heads, hidden size $64$) followed by a GraphSAGE aggregation layer that outputs a $d=32$ dimensional embedding. Each layer is followed by Batch Normalisation and a LeakyReLU activation. The network is trained for $200$ epochs with the Adam optimiser (learning rate $10^{-3}$, weight decay $10^{-4}$) using a mini--batch size of $128$ nodes obtained via neighbour sampling. We set the loss weight to $\gamma = 0.5$. These hyper-parameters were selected based on preliminary sweeps that balanced embedding quality and training time.

    Once the final embeddings are learned we apply Hierarchical Agglomerative Clustering with Ward's linkage. This produces a dendrogram which we cut at multiple levels to generate a diverse set $\mathcal{C} = \{C_1, C_2, \dots, C_M\}$ of candidate supergeo partitions.

    \subsection{Stage 2: Heterogeneity-Aware Optimal Partitioning}
    The second stage selects and partitions the best set of candidate supergeos from $\mathcal{C}$. We formulate this as a set partitioning problem. Let $z_j$ be a binary variable indicating if candidate partition $C_j \in \mathcal{C}$ is chosen and let $y_{sk}$ be a binary variable that is 1 if supergeo $s \in C_j$ is assigned to group $k \in \{T, C\}$. The optimisation problem is to select a partition $C_j$ and assign its supergeos to groups to minimise our objective function subject to constraints.

    The objective function balances on both the baseline outcome and key treatment effect modifiers using the Standardised Mean Difference (SMD). We use the SMD because it is a scale-free metric, which allows for the principled balancing of covariates with different native units and variances (e.g., population versus median income). The SMD for a variable $X$ between two groups $A$ and $B$ is defined as \[
    \text{SMD}(X) = \frac{\bar{X}_A - \bar{X}_B}{\sqrt{(\sigma_A^{2} + \sigma_B^{2})/2}}.
\] The objective is
    \[
\min_{z, y} \left( \text{SMD}(\text{Baseline}) + \sum_{m} \lambda_m\, \text{SMD}(\text{Modifier}_m) \right).
\]
    subject to
    $$ \sum_{j=1}^{M} z_j = 1 \quad \text{(select exactly one partition)} $$
    $$ \sum_{k \in \{T,C\}} y_{sk} = 1 \quad \forall s \in C_j \text{ if } z_j=1 \quad \text{(assign each supergeo to one group)} $$
    This problem is solved efficiently using the Google OR-Tools CP-SAT solver. By creating groups that are well-balanced on the distributions of these key covariates we produce a design that is explicitly prepared for a subsequent high-precision distributional treatment effect analysis.

    \subsection{Hyperparameter Tuning}
    The choice of the regularisation parameters $\lambda_m$ is critical to the performance of the design. We perform a $K{=}5$-fold cross-validation over a logarithmic grid of $\lambda_m$ values. For each candidate vector $\boldsymbol{\lambda}$ we: (i) generate a design on the training folds and (ii) estimate the \emph{expected RMSE of the iROAS estimator} on the held-out fold via a fast Monte-Carlo power simulation. As a tie-breaker we also monitor the mean absolute SMD (MASMD) across modifiers to ensure that a low predicted RMSE is not obtained at the cost of severe imbalance. The selected $\boldsymbol{\lambda}^*$ lies on the Pareto frontier of (RMSE, MASMD) and minimises the RMSE criterion.


\section{Theoretical Properties}
\label{sec:theory}
While the Adaptive Supergeo Design methodology is a heuristic approach to an NP-hard problem it is important to establish a theoretical foundation for its performance. Our theoretical argument rests on the idea that if geographic units form well-separated clusters, our GNN can effectively identify these 'correct' building blocks. If the GNN finds these blocks, the subsequent optimization is much more likely to find a near-optimal solution. In this section we formalise this intuition by outlining the assumptions under which our method is expected to perform well and presenting a guarantee for the quality of the solutions it produces.

    \subsection{Assumptions}
    Our theoretical guarantees rely on two primary assumptions regarding the structure of the geographic data and the behaviour of the learned embedding function.

    \begin{assumption}[Community Structure]\label{assump:community}
    Let $\mathcal{G} = (V, E)$ be the graph of geographic units. We assume there exists a ground-truth partition of the vertices $V$ into disjoint communities $V_1, \dots, V_K$. The graph exhibits a $(\rho_{in}, \rho_{out})$-community structure if for any node $v \in V_c$, the probability of an edge connecting $v$ to another node in $V_c$ is at least $\rho_{in}$, and the probability of an edge to a node in $V_{c'}$ for $c' \neq c$ is at most $\rho_{out}$, where $\rho_{in} > \rho_{out}$.
    \end{assumption}
    
    This is an intuitive assumption that formalises the tendency for geographic units to form regional clusters that share economic and demographic characteristics.

    \begin{assumption}[Lipschitz Continuity]
    Let $f: \mathcal{X} \to \mathbb{R}^d$ be the GNN embedding function that maps node features to embeddings. We assume there exists a constant $L > 0$ such that for any two feature vectors $X_i, X_j \in \mathcal{X}$, the function is Lipschitz continuous.
    $$ \|f(X_i) - f(X_j)\| \le L \|X_i - X_j\| $$
    This is a standard assumption for well-behaved neural networks and it ensures that the learned space is smooth and preserves local similarities from the original feature space.
    \end{assumption}

    \subsection{Approximation Guarantees}
    Under these assumptions we can provide a probabilistic guarantee on the quality of the solution found by our two-stage heuristic relative to the true but computationally unobtainable optimal solution. Let a partition of the geos into treatment and control groups be denoted by $\mathcal{P} = (\mathcal{G}_T, \mathcal{G}_C)$. We define the cost of a partition as the value of our heterogeneity-aware objective function
    $$ \text{Cost}(\mathcal{P}) = \text{SMD}(\text{Baseline}; \mathcal{P}) + \sum_{m} \lambda_m \cdot \text{SMD}(\text{Modifier}_m; \mathcal{P}) $$
    Our main theoretical result can then be stated as the following proposition.

    %----------------------------
    % Approximation guarantee
    %----------------------------
    \begin{proposition}\label{prop:approx}
    Let $\mathcal{G}$ be a graph with a $(\rho_{in},\rho_{out})$–community structure and let $f$ be an $L$–Lipschitz embedding. Denote by $\Delta:=\max_{v\in V}\min_{c} \|f(X_v)-\mu_{c}\|$ the worst–case embedding distortion relative to the centroid $\mu_{c}$ of the true community containing $v$. Define
    \[
        \varepsilon := \kappa_1\,\frac{\rho_{out}}{\rho_{in}-\rho_{out}} + \kappa_2\,L\,\Delta, \quad \text{for constants }\kappa_1,\kappa_2>0.
    \]
    Let $\mathcal{P}_{opt}$ be the partition that minimises $\text{Cost}(\mathcal{P})$ and let $\mathcal{P}_{ASD}$ be the partition returned by Adaptive Supergeo Design. Then, with probability at least $1-\exp\bigl(-cN(\rho_{in}-\rho_{out})^{2}\bigr)$ over the randomness of (i) GNN parameter initialisation and mini–batch sampling and (ii) stochasticity of hierarchical clustering cuts, we have
    $$
        \text{Cost}(\mathcal{P}_{ASD}) \le (1+\varepsilon)\,\text{Cost}(\mathcal{P}_{opt}).
    $$
    The bound tightens as communities become more separated (large $\rho_{in}-\rho_{out}$) and embeddings become smoother (small $L$) or more concentrated (small $\Delta$).
    \end{proposition}

    In essence, the result says that ASD is near–optimal whenever geographic regions are naturally distinct and the learned embeddings respect this structure. Importantly, conditional on the candidate set $\mathcal{C}$ produced in Stage 1, the CP--SAT integer program solved in Stage 2 is \emph{globally optimal}. Thus all residual approximation error arises solely from any omission of the truly optimal supergeo partition from $\mathcal{C}$. The proposition therefore characterises how structural properties of the graph and the embedding quality jointly control this error.

    A formal proof is deferred to Appendix~\ref{app:proof_prop1}. The argument proceeds via a sequence of lemmas. First we show that the GNN embedding function $f$ maps nodes from the same underlying community to a compact region in the embedding space. Second we show that with high probability Hierarchical Agglomerative Clustering recovers the true communities from these embeddings. Third we bound the cost inflation that can be introduced by any clustering errors. These steps collectively establish that our heuristic operates on a high-quality set of candidate supergeos which allows the final optimisation stage to find a solution provably close to the true optimum.

\section{Simulation Studies}
\label{sec:simulations}
\input{experiments}
To evaluate the performance of the Adaptive Supergeo Design we conduct a series of simulation studies. These studies are designed to test the methodology's scalability its accuracy under different conditions and its robustness to violations of its underlying assumptions.

    \subsection{Experimental Setup}
    We construct a synthetic world of $N$ geographic units. For each unit $i$ we generate a vector of pre-treatment covariates $X_i$ and a baseline response level $Y_i(0)$. The core of our simulation is the data generating process for the individual treatment effects $\tau_i$. We model these effects as a function of the covariates allowing us to precisely control the degree and nature of the treatment effect heterogeneity. In our primary simulation the effects are generated according to the linear model $\tau_i = c + w^T X_i + \eta_i$ where $c$ is a baseline effect $w$ is a vector of weights that determines how the covariates drive heterogeneity and $\eta_i$ is an irreducible noise term.

    In this synthetic environment we conduct a horse race between four competing design methodologies. The first is the original Trimmed Match Design or TMD. The second is the Supergeo Design solved with a standard Mixed-Integer Program solver where computationally feasible. The third is our Adaptive Supergeo Design without heterogeneity-aware regularisation. The fourth is the full ASD with the regularisation enabled.

    We evaluate the performance of each design based on three key metrics. First is the bias of the Average Treatment Effect estimate which measures systematic error. Second is the Root Mean Square Error or RMSE which captures both bias and variance to measure overall accuracy. Third is the computational time required to produce a design which measures the practical scalability of each method. Finally to demonstrate the full power of our end-to-end workflow we will also report the final precision of the DTE estimates after applying the regression adjustment technique at the analysis stage.

    \subsection{Results on Scalability}
    To assess scalability we run each methodology on datasets of increasing size from $N=50$ to $N=5000$ geos. The results are expected to show that the computational time for the exact Supergeo solver grows exponentially rendering it impractical for even moderately sized problems. In contrast the runtime for our two-stage ASD heuristic should scale gracefully in a polynomial fashion demonstrating its feasibility for large-scale real-world applications.

    \subsection{Results on Homogeneous Treatment Effects}
    We first test the simple scenario where the treatment effect is homogeneous across all units which corresponds to setting $w=0$ in our data generating process. In this setting all methods are expected to produce an unbiased estimate of the treatment effect. The primary differentiator will be the variance of the estimates. We anticipate that ASD will perform comparably to the other methods demonstrating that our approach does not sacrifice efficiency in the absence of the heterogeneity it is designed to address.

    \subsection{Results on Heterogeneous Treatment Effects}
    This simulation constitutes the main test of our proposed methodology. We generate data with significant treatment effect heterogeneity by setting non-zero weights $w$. The results are expected to highlight the strengths of our approach. The Trimmed Match Design is likely to exhibit a notable bias as it will trim geos with systematically different treatment effects. Our ASD method without regularisation should be unbiased but may have higher variance. The full ASD with heterogeneity-aware regularisation is hypothesised to achieve both the lowest bias and the lowest overall RMSE by correctly balancing on the covariates that drive the heterogeneity.

    \subsection{Robustness Checks and Stress Tests}
    Finally we evaluate the robustness of our method under more challenging conditions where its core assumptions are violated. We run three additional scenarios. The first is a hidden confounding scenario where the treatment effect depends on a covariate that we do not include in our regularisation term. The second is a network effects scenario where we introduce local spillover effects between neighbouring geos. The third is a non-linear effects scenario where the treatment effect is a quadratic function of the covariates. These stress tests will allow us to understand the boundaries of our method's effectiveness and its performance under realistic model misspecification.

    \subsection{End-to-End Performance Evaluation}
    To illustrate the full value of combining state-of-the-art design with state-of-the-art analysis we conduct a final simulation. We compare the final RMSE of the DTE estimates under two scenarios. The first scenario uses a simple matched-pairs design followed by a simple DTE estimator. The second scenario uses our full ASD design followed by the ML-based regression adjustment analysis. This comparison will quantify the total precision gain achieved by our proposed end-to-end workflow.



\section{Conclusion and Future Work}
\label{sec:conclusion}
In this paper we introduced the Adaptive Supergeo Design a novel framework for creating scalable and robust geographic experiments. We addressed the twin challenges of computational intractability and treatment effect heterogeneity that have limited the application of previous advanced design methodologies. Our primary contribution is the design component of a powerful end-to-end workflow. We propose a two-stage heuristic that leverages graph neural networks and combinatorial optimisation to create optimally balanced experimental groups. This design is explicitly constructed to be paired with state-of-the-art regression adjustment techniques at the analysis stage to enable the precise and robust estimation of distributional treatment effects.

The ASD framework offers significant advantages over existing methods. As demonstrated in our simulation studies its two-stage approach scales gracefully to problems involving thousands of geographic units making it a practical tool for real-world large-scale experiments. Furthermore by explicitly balancing on likely treatment effect modifiers our methodology produces designs that are more robust to heterogeneity. This leads to more credible and representative estimates of the average treatment effect particularly in complex settings where the impact of a campaign is not uniform across the population.

We also acknowledge the limitations of our approach. The performance of the candidate generation stage is contingent on the quality and richness of the input features. While we have proposed a sophisticated feature engineering process the model cannot account for unobserved sources of heterogeneity. Our theoretical guarantees also rest on assumptions about the structure of the data that may not always be perfectly met in practice. The selection of which covariates to treat as effect modifiers also requires careful consideration and domain expertise.

This work opens up several exciting avenues for future research. One promising direction is the development of dynamic experimental designs where the supergeo partitions could be updated over time as more data becomes available during an experiment. Another area of interest is the extension of the ASD framework to handle experiments with multiple treatment arms which is a common requirement in many applied settings. A particularly important extension would be to move beyond estimating average treatment effects to understanding distributional treatment effects. This would involve modifying the objective function to minimise a measure of distributional distance such as the Wasserstein distance or the Kolmogorov-Smirnov statistic instead of just the difference in means. Such a design would provide a much richer picture of the campaign's impact. Finally exploring end-to-end deep learning models that might learn to produce optimal partitions directly without a two-stage process could be a fruitful though challenging research path.




\appendix
\section{Proof Sketch of Proposition~\ref{prop:approx}}\label{app:proof_prop1}
\paragraph{Outline.} We give a concise but fully formal argument.  Let $\mathcal{C}^{\star}$ denote the unknown ground-truth community partition and let $\mu_{c}:=\mathbb E\bigl[f(X_v)\,\big|\,v\in c\bigr]$ be the population centroid of community $c\in\mathcal{C}^{\star}$.  Recall the worst-case embedding distortion $\Delta:=\max_{v}\min_{c}\|f(X_v)-\mu_{c}\|$ and the community parameters $(\rho_{in},\rho_{out})$ from Assumption~\ref{assump:community}.

\begin{lemma}[Within--community concentration]\label{lem:conc}
With probability at least $1-\exp\!\bigl(-c_1|c|\,\Delta^{2}\bigr)$ the empirical mean embedding \,$\bar h_{c}:=|c|^{-1}\sum_{v\in c}f(X_v)$\, satisfies 
\[\|\bar h_{c}-\mu_{c}\|_2\le\Delta.\]
\end{lemma}
\begin{proof}[Idea]
Apply Hoeffding bounds to the random adjacency matrix, invoke the $L$–Lipschitz property of $f$, and conclude with McDiarmid’s inequality.
\end{proof}

\begin{lemma}[Centroid separation]\label{lem:sep}
For any distinct communities $c\neq c'$, with probability at least $1-\exp\!\bigl(-c_2 N(\rho_{in}-\rho_{out})^{2}\bigr)$,
\[\|\bar h_{c}-\bar h_{c'}\|_2\ge\rho_{in}-\rho_{out}-2\Delta.\]
\end{lemma}

\begin{lemma}[Hierarchical clustering fidelity]\label{lem:hac}
If $\rho_{in}-\rho_{out}>2\Delta$, Ward–linkage HAC exactly recovers $\mathcal{C}^{\star}$ (Theorem~3 of \citealp{vonLuxburg2010}).
\end{lemma}

\begin{lemma}[Bounding design cost]\label{lem:cost}
Condition on the event of Lemma~\ref{lem:hac}.  Then the candidate set $\mathcal{C}$ produced in Stage~1 contains a partition $\tilde{\mathcal P}$ such that
\[\text{Cost}(\tilde{\mathcal P})\le(1+\varepsilon)\,\text{Cost}(\mathcal P_{opt}),\]
where $\varepsilon$ is exactly the quantity defined in Proposition~\ref{prop:approx}.
\end{lemma}
\begin{proof}[Sketch]
Mis-clustered nodes alter each Standardised Mean Difference term by at most $L\,\Delta$.  Summing these perturbations and normalising yields the additive contribution $\kappa_{2}L\,\Delta$.  Imperfect community separation adds $\kappa_{1}\rho_{out}/(\rho_{in}-\rho_{out})$.  Together these give $\varepsilon$.
\end{proof}

\paragraph{Putting everything together}  Take a union bound over the failure probabilities in Lemmas~\ref{lem:conc}–\ref{lem:cost}.  With overall probability at least $1-\exp\!\bigl(-c N(\rho_{in}-\rho_{out})^{2}\bigr)$,
\[\text{Cost}(\mathcal P_{ASD})\le(1+\varepsilon)\,\text{Cost}(\mathcal P_{opt}),\]
which completes the proof of Proposition~\ref{prop:approx}.  \qed

\bigskip

\noindent\textbf{Remark.} Alternative cost functions that are convex combinations of standardised mean differences can be incorporated with minor algebra; the constants $\kappa_1,\kappa_2$ adjust accordingly.

\vskip 0.2in
\begin{thebibliography}{99}

\bibitem[Abadie et al.(2010)]{abadie2010synthetic}
Abadie, A., Diamond, A. and Hainmueller, J., 2010. Synthetic control methods for comparative case studies: Estimating the effect of California’s tobacco control program. \textit{Journal of the American statistical Association}, 105(490), pp.493-505.

\bibitem[Athey and Imbens(2006)]{athey2006identification}
Athey, S. and Imbens, G.W., 2006. Identification and inference in nonlinear difference-in-differences models. \textit{Econometrica}, 74(2), pp.431-497.

\bibitem[Athey and Imbens(2017)]{athey2017econometrics}
Athey, S. and Imbens, G.W., 2017. The state of applied econometrics: Causality and policy evaluation. \textit{Journal of Economic Perspectives}, 31(2), pp.3-32.

\bibitem[Bickel et al.(1993)]{bickel1993efficient}
Bickel, P.J., Klaassen, C.A., Ritov, Y. and Wellner, J.A., 1993. \textit{Efficient and adaptive estimation for semiparametric models}. Springer.

\bibitem[Byambadalai et al.(2024)]{byambadalai2024}
Byambadalai, U., Hirata, T., Oka, T. and Yasui, S., 2024. On Efficient Estimation of Distributional Treatment Effects under Covariate-Adaptive Randomization. \textit{arXiv preprint arXiv:2506.05945}.

\bibitem[Callaway(2021)]{callaway2021bounds}
Callaway, B., 2021. Bounds on the distribution of treatment effects with a stochastic monotone instrument. \textit{Quantitative Economics}, 12(3), pp.931-963.

\bibitem[Chernozhukov et al.(2018)]{chernozhukov2018debiased}
Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W. and Robins, J., 2018. Double/debiased machine learning for treatment and structural parameters. \textit{The Econometrics Journal}, 21(1), pp.C1-C68.

\bibitem[Chen et al.(2021)]{chen2021} 
Chen, A., Longfils, M. and Remy, N., 2021. Trimmed Match Design for Randomized Paired Geo Experiments. \textit{arXiv preprint arXiv:2105.07060}.

\bibitem[Chen et al.(2023)]{chen2023} 
Chen, A., Doudchenko, N., Jiang, S., Stein, C. and Ying, B., 2023. Supergeo Design: Generalized Matching for Geographic Experiments. \textit{arXiv preprint arXiv:2301.12044}.

\bibitem[Doudchenko et al.(2021)]{doudchenko2021synthetic}
Doudchenko, N., Khosravi, K., Pouget-Abadie, J., Lahaie, S., Lubin, M., Mirrokni, V. and Spiess, J., 2021. Synthetic design: An optimization approach to experimental design with synthetic controls. \textit{Advances in Neural Information Processing Systems}, 34, pp.8691-8701.

\bibitem[Duflo et al.(2007)]{duflo2007using}
Duflo, E., Glennerster, R. and Kremer, M., 2007. Using randomization in development economics research: A toolkit. \textit{Handbook of development economics}, 4, pp.3895-3962.

\bibitem[Edmonds(1965)]{edmonds1965maximum}
Edmonds, J., 1965. Maximum matching and a polyhedron with 0, 1-vertices. \textit{Journal of research of the National Bureau of Standards B}, 69(125-130), pp.55-56.

\bibitem[Holland(1986)]{holland1986statistics}
Holland, P.W., 1986. Statistics and causal inference. \textit{Journal of the American statistical Association}, 81(396), pp.945-960.

\bibitem[Imbens and Rubin(2015)]{imbens2015causal}
Imbens, G.W. and Rubin, D.B., 2015. \textit{Causal inference for statistics, social, and biomedical sciences: An introduction}. Cambridge University Press.

\bibitem[Kunzel et al.(2019)]{kunzel2019metalearners}
K\"{u}nzel, S.R., Sekhon, J.S., Bickel, P.J. and Yu, B., 2019. Metalearners for estimating heterogeneous treatment effects using machine learning. \textit{Proceedings of the national academy of sciences}, 116(10), pp.4156-4165.

\bibitem[Lin(2013)]{lin2013agnostic}
Lin, W., 2013. Agnostic notes on regression adjustments to experimental data: Reexamining Freedman's critique. \textit{The Annals of Applied Statistics}, 7(1), pp.295-318.

\bibitem[Oka et al.(2024)]{oka2024regression}
Oka, T., Yasui, S., Hayakawa, Y. and Byambadalai, U., 2024. Regression Adjustment for Estimating Distributional Treatment Effects in Randomized Controlled Trials. \textit{arXiv preprint arXiv:2407.14074}.

\bibitem[Papadimitriou and Steiglitz(1998)]{papadimitriou1998combinatorial}
Papadimitriou, C.H. and Steiglitz, K., 1998. \textit{Combinatorial optimization: algorithms and complexity}. Courier Corporation.

\bibitem[Rosenbaum(2020)]{rosenbaum2020design}
Rosenbaum, P.R., 2020. \textit{Design of observational studies}. Springer.

\bibitem[Stuart(2010)]{stuart2010matching}
Stuart, E.A., 2010. Matching methods for causal inference: A review and a look forward. \textit{Statistical science}, 25(1), p.1.

\bibitem[Vaver and Koehler(2011)]{vaver2011measuring}
Vaver, J. and Koehler, J., 2011. Measuring ad effectiveness using geo experiments. \textit{Google Research}.

\bibitem[Zubizarreta(2012)]{zubizarreta2012using}
Zubizarreta, J.R., 2012. Using mixed integer programming for matching in an observational study of kidney failure after surgery. \textit{Journal of the American Statistical Association}, 107(500), pp.1360-1371.

\bibitem[Gordon et al.(2021)]{gordon2021advertising}
Gordon, B.R., Jerath, K., Katona, Z., Narayanan, S., Shin, J., and Wilbur, K.C., 2021. Inefficiencies in digital advertising markets. \textit{Journal of Marketing}, 85(1), pp.7--25.

\bibitem[von Luxburg(2010)]{vonLuxburg2010}
von Luxburg, U., 2010. Clustering stability: an overview. \textit{Foundations and Trends in Machine Learning}, 2(3), pp.235--274.

\end{thebibliography}


\appendix
\section{Practical Guidance for Deploying ASD}\label{sec:practical}
To support adoption by marketing practitioners and media agencies we distil our experience into a concise deployment checklist.

\paragraph{Data requirements} ASD operates exclusively on \emph{geo\,-aggregated} data. The minimum inputs are:(i) a time series of weekly revenue $R_i^t$ and spend $S_i^t$ for each geo $i$ over a historical window (\,$\geq$~26 weeks is recommended for seasonality coverage), and (ii) static area\,-level covariates such as population, median income, and digital penetration rates. No user\,-level or personally identifiable information (PII) is required.

\paragraph{Recommended defaults} For most campaigns we suggest: 2~GAT layers (8~heads, 64~hidden) $\rightarrow$ GraphSAGE (32~dim), $\gamma=0.5$, embedding dimension $d=32$, candidate set size $M=100$, cross\,-validation folds $K=5$, and a logarithmic grid of 10~$\lambda_m$ values in $[10^{-3},1]$.

\paragraph{Deployment workflow} (1) Export weekly geo metrics from the ad platform's data warehouse. (2) Run the \texttt{asd\_design.py} script with historical data to produce the treatment\,/control assignment. (3) Share the resulting CSV with the trafficking team to configure budget multipliers. (4) After the campaign, feed observed outcomes to the accompanying \texttt{asd\_analysis.py} module to estimate iROAS.

\paragraph{Privacy \,\& compliance} Because ASD consumes only aggregated geo\,-level signals, no PII ever leaves the advertiser's environment. This design is compatible with GDPR, CPRA, and similar regulations, and avoids the consent management overhead associated with user\,-level experiments.

\paragraph{Open\,-source tooling} Python code is available at \url{https://github.com/shawcharles/asd}. %Practitioners can replicate every figure in this paper with a single bash script.


\end{document}
